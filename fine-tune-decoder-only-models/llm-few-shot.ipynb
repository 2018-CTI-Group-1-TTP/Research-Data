{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install transformers torch bitsandbytes pandas\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","from dotenv import SECRET_HF\n","secret_hf = SECRET_HF\n","!huggingface-cli login --token $secret_hf"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import bitsandbytes as bnb\n","import pandas as pd\n","\n","\n","# Load the tokenizer\n","model_name = \"mistralai/Mistral-7B-v0.1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","# Load the model with 4-bit quantization\n","model = AutoModelForCausalLM.from_pretrained(model_name, \n","                                             quantization_config=bnb_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import re\n","\n","# Function to process chunks of text\n","def process_chunk(prompt, model, tokenizer, max_len):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model.generate(inputs.input_ids, max_length=max_len, num_return_sequences=1, temperature=0.7)\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return generated_text\n","\n","\n","    \n","    \n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import re\n","import gc\n","\n","\n","def get_gpu_free_memory():\n","    total_memory = torch.cuda.get_device_properties(0).total_memory\n","    reserved_memory = torch.cuda.memory_reserved(0)\n","    allocated_memory = torch.cuda.memory_allocated(0)\n","    free_memory = total_memory - (reserved_memory + allocated_memory)\n","    return free_memory\n","\n","test_df = pd.read_csv('/kaggle/input/mistral-dataset/test.csv')\n","num_of_rows = test_df.shape[0]\n","print(\"Total testing datapoints: \",num_of_rows)\n","\n","predicted_json = []\n","testing_count = 0\n","\n","# Read the CSV file\n","df = pd.read_csv('/kaggle/input/few-shot-learning/train_modified.csv')\n","prompt = \"Task: You are a cyber security specialist. Provide the list of MITRE techniques indicated in the context. Do not need to specify any reasoning, just provide a comma separated list. Choose the techniques among these 15 techniques: T1059 Command and Scripting interpreter, T1003 OS Credential Dumping, T1486 Data Encrypted for Impact, T1055 Process Injection, T1082 System Information Discovery, T1021 Remote Services, T1047 Windows Management Instrumentation, T1053 Scheduled Task/Job, T1497 Virtualization/Sandbox Evasion, T1018 Remote System Discovery, T1566 Phishing, T1027 Obfuscated Files or information, T1105 Ingress tool transfer, T1562 Impair defenses, T0814 Denial of Service.\\n\\n\"\n","\n","count = 0\n","for i in range(500):\n","    data = df.iloc[i]\n","    input = data['input']\n","    if len(input) <= 500 and len(input) >= 300:\n","        prompt +=  f\"Context: {data['input']}\\nQuestion: What are the MITRE techniques indicated in the threat report?\\nAnswer: {data['output']}\\n\\n\"\n","        count += 1\n","    if count == 3:\n","        break\n","#     print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# generated_text = process_chunk(few_shot_prompt, model, tokenizer)\n","# print(f\"Sample {index + 1}:\\n{generated_text}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(0, num_of_rows):\n","\n","\n","    row_df = test_df.iloc[i]\n","\n","    #print(df)\n","\n","    #take instruction and input column\n","    #instruction = row_df['instruction']\n","    input = row_df['input']\n","    output = row_df['output']\n","\n","\n","\n","    text = input\n","    text = text.replace('\\n','')\n","    text = text.replace('\\r\\n','')\n","    text = str(text)\n","    text = f\"Context: {input}\\nQuestion: What are the MITRE techniques indicated in the threat report?\\nAnswer: \" #to be changed\n","    \n","    print()\n","    print(\"GPU free memory available: \",get_gpu_free_memory()/(1024*1024*1024),\" GB\")\n","    \n","    print(\"Processing \",i+1,\"th datapoint.Text size(in words): \",len(text.split(\" \")))\n","    \n","\n","    if(len(text.split(\" \")) > 2000):\n","         print(\"Ignored \",i+1,\"th datapoint.Text size(in words): \",len(text.split(\" \")))        \n","         continue\n","\n","\n","    few_shot_prompt = prompt + text\n","    max_len = len(few_shot_prompt)\n","    \n","    generated_text = process_chunk(few_shot_prompt, model, tokenizer, max_len)\n","    \n","    response = generated_text\n","    # Regular expression to extract the answer\n","    print(response)\n","    # Regular expression to extract the last \"Answer\"\n","    \n","    print()\n","    print()\n","    print()\n","    \n","    matches = re.findall(r'Answer:\\s*(.*)', response)\n","    if len(matches) > 3:\n","        fourth_answer = matches[3]\n","        print(f\"4th Answer: {fourth_answer}\")\n","    else:\n","        print(\"There are less than 4 answers in the context.\")\n","        continue\n","   \n","    responded_techniques = fourth_answer.split(\",\")\n","\n","    responded_techniques_list =[]\n","    for st in responded_techniques:\n","        st = st.replace('\\n','')\n","        st = st.replace('\\r\\n','')\n","        st = st.strip()\n","        responded_techniques_list.append(st)\n","\n","\n","    predicted_techniques = ','.join(responded_techniques_list)\n","    #print(predicted_techniques)\n","    #break\n","    \n","    predicted_json.append({\n","        \"Article\" : input,\n","        \"Actual_output\" : output,\n","        \"Predicted_output\" : predicted_techniques\n","\n","    })\n","    print(\"Testing for \",i+1,\"th datapoints done.\")\n","    \n","    testing_count = testing_count + 1\n","\n","    predicted_df=pd.DataFrame(predicted_json)\n","    #predicted_df\n","    predicted_df.to_csv('/kaggle/working/mistral_few_shot.csv', index=False)\n","    print(\"Storing \",i+1,\"datapoints to csv.Text size(in words): \",len(text.split(\" \")))\n","    gc.collect()\n","\n","\n","print(\"Tested datapoint count: \",testing_count)\n","print(\"test_evaluation.csv is created successfully\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4395990,"sourceId":7548020,"sourceType":"datasetVersion"},{"datasetId":4674423,"sourceId":7948914,"sourceType":"datasetVersion"},{"datasetId":5149805,"sourceId":8606347,"sourceType":"datasetVersion"}],"dockerImageVersionId":30716,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
